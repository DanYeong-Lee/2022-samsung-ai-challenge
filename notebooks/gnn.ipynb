{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/project/danyoung/miniconda3/envs/mol/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from src.datamodules.components.dataset2d import TrainDataset, TestDataset\n",
    "from src.datamodules.datamodule2d import BaseDataModule\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123456789)\n",
    "                for i, (train_idx, val_idx) in enumerate(kfold.split(self.full_data)):\n",
    "                    if i == self.hparams.fold:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "\n",
    "from ogb.utils.features import (allowable_features, atom_to_feature_vector,\n",
    " bond_to_feature_vector, atom_feature_vector_to_dict, bond_feature_vector_to_dict) \n",
    "\n",
    "\n",
    "def mol2graph(mol):\n",
    "    \"\"\"\n",
    "    Converts SMILES string to graph Data object\n",
    "    :input: SMILES string (str)\n",
    "    :return: graph object\n",
    "    \"\"\"\n",
    "\n",
    "    # atoms\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features_list.append(atom_to_feature_vector(atom))\n",
    "    x = np.array(atom_features_list, dtype = np.int64)\n",
    "\n",
    "    # bonds\n",
    "    num_bond_features = 3  # bond type, bond stereo, is_conjugated\n",
    "    if len(mol.GetBonds()) > 0: # mol has bonds\n",
    "        edges_list = []\n",
    "        edge_features_list = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "\n",
    "            edge_feature = bond_to_feature_vector(bond)\n",
    "\n",
    "            # add edges in both directions\n",
    "            edges_list.append((i, j))\n",
    "            edge_features_list.append(edge_feature)\n",
    "            edges_list.append((j, i))\n",
    "            edge_features_list.append(edge_feature)\n",
    "\n",
    "        # data.edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
    "        edge_index = np.array(edges_list, dtype = np.int64).T\n",
    "\n",
    "        # data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "        edge_attr = np.array(edge_features_list, dtype = np.int64)\n",
    "\n",
    "    else:   # mol has no bonds\n",
    "        edge_index = np.empty((2, 0), dtype = np.int64)\n",
    "        edge_attr = np.empty((0, num_bond_features), dtype = np.int64)\n",
    "\n",
    "    return x, edge_attr, edge_index\n",
    "\n",
    "\n",
    "def get_coordinate_features(mol):\n",
    "    conf = mol.GetConformer()\n",
    "    return conf.GetPositions()\n",
    "\n",
    "def get_mol_data(root, prefix, y=None):\n",
    "    if prefix.startswith(\"train\"):\n",
    "        set_dir = \"train_set\"\n",
    "    else:\n",
    "        set_dir = \"test_set\"\n",
    "        \n",
    "    ex = Chem.MolFromMolFile(f\"{root}/{set_dir}/{prefix}_ex.mol\", removeHs=False)\n",
    "    g = Chem.MolFromMolFile(f\"{root}/{set_dir}/{prefix}_g.mol\", removeHs=False)\n",
    "    \n",
    "    # Atom features\n",
    "    X, edge_attr, edge_index = mol2graph(ex)\n",
    "    \n",
    "    # Atom 3D coordinates\n",
    "    co_ex = get_coordinate_features(ex)\n",
    "    co_g = get_coordinate_features(g)\n",
    "            \n",
    "    X = np.concatenate([X, co_ex, co_g], axis=1)\n",
    "    \n",
    "    X = torch.tensor(X, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    y = torch.tensor([y], dtype=torch.float)\n",
    "            \n",
    "    return Data(x=X, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        \n",
    "\n",
    "def get_datalist(df, root):\n",
    "    data_list = []\n",
    "    if \"Reorg_g\" in df.columns:\n",
    "        for _, item in tqdm(df.iterrows()):\n",
    "            y = [item.Reorg_g, item.Reorg_ex]\n",
    "            data = get_mol_data(root, item[0], y)\n",
    "            data_list.append(data)\n",
    "    else:\n",
    "        for _, item in tqdm(df.iterrows()):\n",
    "            data = get_mol_data(root, item[0])\n",
    "            data_list.append(data)\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "\n",
    "class TrainDataset(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=\"/data/project/danyoung/reorg/data/mol_files\",\n",
    "        fold: Union[int, str] = 0,\n",
    "        train: bool = True,\n",
    "        transform=None,\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    ):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        data, slices = torch.load(self.processed_paths[0])\n",
    "        self.data, self.slices = data, slices\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        mol_list = os.listdir(os.path.join(self.root, \"train_set\"))\n",
    "        mol_list = [os.path.join(self.root, \"train_set\", file) for file in mol_list]\n",
    "            \n",
    "        return mol_list\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"2d_dataset_train.pt\"]\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        df = pd.read_csv(f\"{self.root}/../train_set.ReorgE.csv\")\n",
    "        data_list = get_datalist(df, self.root)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "    \n",
    "class TestDataset(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=\"/data/project/danyoung/reorg/data/mol_files\", \n",
    "        transform=None,\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    ):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        mol_list = os.listdir(os.path.join(self.root, \"test_set\"))\n",
    "        mol_list = [os.path.join(self.root, \"test_set\", file) for file in mol_list]\n",
    "            \n",
    "        return mol_list\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"2d_dataset_test.pt\"]\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        df = pd.read_csv(f\"{self.root}/../test_set.csv\")\n",
    "        data_list = get_datalist(df, self.root)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import LightningDataset\n",
    "\n",
    "def get_datamodule(train_data, test_data, fold=0, batch_size=32, num_workers=4):\n",
    "    if type(fold) != str:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "\n",
    "class GATNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 15,\n",
    "        gat_hidden_dim: int = 64,\n",
    "        edge_dim: int = 3,\n",
    "        heads: int = 4,\n",
    "        n_gat_layers: int = 3,\n",
    "        n_fc_layers: int = 3,\n",
    "        fc_hidden_dim: int = 256,\n",
    "        fc_dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gat1 = gnn.GATv2Conv(in_channels=input_dim, \n",
    "                                  out_channels=gat_hidden_dim, heads=heads, edge_dim=edge_dim)\n",
    "        self.gat_list = nn.ModuleList([\n",
    "            gnn.GATv2Conv(in_channels=gat_hidden_dim*heads, \n",
    "                          out_channels=gat_hidden_dim, heads=heads, edge_dim=edge_dim)\n",
    "            for _ in range(n_gat_layers - 1)\n",
    "        ])\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(gat_hidden_dim * heads, fc_hidden_dim),\n",
    "            nn.BatchNorm1d(fc_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fc_dropout)\n",
    "        )\n",
    "        self.fc_list = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(fc_hidden_dim, fc_hidden_dim),\n",
    "                nn.BatchNorm1d(fc_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(fc_dropout)\n",
    "            ) for _ in range(n_fc_layers - 2)\n",
    "        ])\n",
    "        self.do = nn.Dropout(fc_dropout)\n",
    "        self.fc_last = nn.Linear(fc_hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = F.relu(self.gat1(x, edge_index, edge_attr))\n",
    "        \n",
    "        for gat_layer in self.gat_list:\n",
    "            x = gat_layer(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = gnn.global_mean_pool(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        for fc_layer in self.fc_list:\n",
    "            x = fc_layer(x)\n",
    "            \n",
    "        x = self.fc_last(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError, MinMetric\n",
    "\n",
    "\n",
    "class BaseNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-5,\n",
    "        max_epochs: int = 30\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"net\"])\n",
    "        self.net = net\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.train_rmse = MeanSquaredError(squared=False)\n",
    "        self.val_rmse = MeanSquaredError(squared=False)\n",
    "        \n",
    "        self.val_rmse_best = MinMetric()\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        return self.net(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        self.val_rmse_best.reset()\n",
    "        \n",
    "    def step(self, batch):\n",
    "        pred = self(batch)\n",
    "        loss = self.criterion(pred, batch.y)\n",
    "        \n",
    "        return loss, pred, batch.y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred, target = self.step(batch)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, preds, targets = self.step(batch)\n",
    "        self.val_rmse.update(preds, targets)\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # get val metric from current epoch\n",
    "        epoch_rmse = self.val_rmse.compute()\n",
    "        \n",
    "        # log epoch metrics\n",
    "        metrics = {\"val/rmse\": epoch_rmse}\n",
    "        self.log_dict(metrics, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # log best metric\n",
    "        self.val_rmse_best.update(epoch_rmse)\n",
    "        self.log(\"val/rmse_best\", self.val_rmse_best.compute(), on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # reset val metrics\n",
    "        self.val_rmse.reset()\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        _, preds, _ = self.step(batch)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def on_predict_epoch_end(self, outputs):\n",
    "        preds = np.array(torch.cat(outputs[0]))\n",
    "        \n",
    "        sub_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "        sub_df[\"Reorg_g\"] = preds[:, 0]\n",
    "        sub_df[\"Reorg_ex\"] = preds[:, 1]\n",
    "        sub_df.to_csv(\"submission.csv\", sep=\",\", index=False)\n",
    "\n",
    "        print(\"Saved submission file!\")\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        n_steps = len(self.trainer._data_connector._train_dataloader_source.dataloader())\n",
    "        \n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.hparams.max_epochs * n_steps\n",
    "        )\n",
    "        \n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | net           | GATNet           | 408 K \n",
      "1 | criterion     | MSELoss          | 0     \n",
      "2 | train_rmse    | MeanSquaredError | 0     \n",
      "3 | val_rmse      | MeanSquaredError | 0     \n",
      "4 | val_rmse_best | MinMetric        | 0     \n",
      "---------------------------------------------------\n",
      "408 K     Trainable params\n",
      "0         Non-trainable params\n",
      "408 K     Total params\n",
      "1.633     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|███████▉  | 453/567 [00:15<00:03, 28.99it/s, loss=0.129, v_num=4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80%|████████  | 454/567 [00:16<00:03, 28.32it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  80%|████████  | 455/567 [00:16<00:03, 28.34it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  80%|████████  | 456/567 [00:16<00:03, 28.32it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  81%|████████  | 457/567 [00:16<00:03, 28.34it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  81%|████████  | 458/567 [00:16<00:03, 28.36it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  81%|████████  | 459/567 [00:16<00:03, 28.38it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  81%|████████  | 460/567 [00:16<00:03, 28.41it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  81%|████████▏ | 461/567 [00:16<00:03, 28.43it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  81%|████████▏ | 462/567 [00:16<00:03, 28.44it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  82%|████████▏ | 463/567 [00:16<00:03, 28.46it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  82%|████████▏ | 464/567 [00:16<00:03, 28.49it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  82%|████████▏ | 465/567 [00:16<00:03, 28.51it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  82%|████████▏ | 466/567 [00:16<00:03, 28.53it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  82%|████████▏ | 467/567 [00:16<00:03, 28.55it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  83%|████████▎ | 468/567 [00:16<00:03, 28.57it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  83%|████████▎ | 469/567 [00:16<00:03, 28.61it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  83%|████████▎ | 470/567 [00:16<00:03, 28.64it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  83%|████████▎ | 471/567 [00:16<00:03, 28.66it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  83%|████████▎ | 472/567 [00:16<00:03, 28.68it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  83%|████████▎ | 473/567 [00:16<00:03, 28.70it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  84%|████████▎ | 474/567 [00:16<00:03, 28.73it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  84%|████████▍ | 475/567 [00:16<00:03, 28.75it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  84%|████████▍ | 476/567 [00:16<00:03, 28.77it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  84%|████████▍ | 477/567 [00:16<00:03, 28.80it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  84%|████████▍ | 478/567 [00:16<00:03, 28.81it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  84%|████████▍ | 479/567 [00:16<00:03, 28.84it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  85%|████████▍ | 480/567 [00:16<00:03, 28.87it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  85%|████████▍ | 481/567 [00:16<00:02, 28.89it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  85%|████████▌ | 482/567 [00:16<00:02, 28.92it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  85%|████████▌ | 483/567 [00:16<00:02, 28.95it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  85%|████████▌ | 484/567 [00:16<00:02, 28.98it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  86%|████████▌ | 485/567 [00:16<00:02, 29.01it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  86%|████████▌ | 486/567 [00:16<00:02, 29.04it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  86%|████████▌ | 487/567 [00:16<00:02, 29.07it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  86%|████████▌ | 488/567 [00:16<00:02, 29.09it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  86%|████████▌ | 489/567 [00:16<00:02, 29.12it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  86%|████████▋ | 490/567 [00:16<00:02, 29.15it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  87%|████████▋ | 491/567 [00:16<00:02, 29.17it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  87%|████████▋ | 492/567 [00:16<00:02, 29.18it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  87%|████████▋ | 493/567 [00:16<00:02, 29.20it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  87%|████████▋ | 494/567 [00:16<00:02, 29.23it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  87%|████████▋ | 495/567 [00:16<00:02, 29.24it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  87%|████████▋ | 496/567 [00:16<00:02, 29.27it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  88%|████████▊ | 497/567 [00:16<00:02, 29.29it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  88%|████████▊ | 498/567 [00:16<00:02, 29.32it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  88%|████████▊ | 499/567 [00:17<00:02, 29.34it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  88%|████████▊ | 500/567 [00:17<00:02, 29.37it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  88%|████████▊ | 501/567 [00:17<00:02, 29.40it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  89%|████████▊ | 502/567 [00:17<00:02, 29.40it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  89%|████████▊ | 503/567 [00:17<00:02, 29.43it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  89%|████████▉ | 504/567 [00:17<00:02, 29.47it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  89%|████████▉ | 505/567 [00:17<00:02, 29.50it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  89%|████████▉ | 506/567 [00:17<00:02, 29.53it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  89%|████████▉ | 507/567 [00:17<00:02, 29.57it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  90%|████████▉ | 508/567 [00:17<00:01, 29.60it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  90%|████████▉ | 509/567 [00:17<00:01, 29.64it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  90%|████████▉ | 510/567 [00:17<00:01, 29.67it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  90%|█████████ | 511/567 [00:17<00:01, 29.70it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  90%|█████████ | 512/567 [00:17<00:01, 29.73it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  90%|█████████ | 513/567 [00:17<00:01, 29.76it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  91%|█████████ | 514/567 [00:17<00:01, 29.79it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  91%|█████████ | 515/567 [00:17<00:01, 29.82it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  91%|█████████ | 516/567 [00:17<00:01, 29.85it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  91%|█████████ | 517/567 [00:17<00:01, 29.87it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  91%|█████████▏| 518/567 [00:17<00:01, 29.90it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  92%|█████████▏| 519/567 [00:17<00:01, 29.92it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  92%|█████████▏| 520/567 [00:17<00:01, 29.94it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  92%|█████████▏| 521/567 [00:17<00:01, 29.96it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  92%|█████████▏| 522/567 [00:17<00:01, 29.98it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  92%|█████████▏| 523/567 [00:17<00:01, 30.00it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  92%|█████████▏| 524/567 [00:17<00:01, 30.03it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  93%|█████████▎| 525/567 [00:17<00:01, 30.05it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  93%|█████████▎| 526/567 [00:17<00:01, 30.06it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  93%|█████████▎| 527/567 [00:17<00:01, 30.07it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  93%|█████████▎| 528/567 [00:17<00:01, 30.09it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  93%|█████████▎| 529/567 [00:17<00:01, 30.11it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  93%|█████████▎| 530/567 [00:17<00:01, 30.13it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  94%|█████████▎| 531/567 [00:17<00:01, 30.15it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  94%|█████████▍| 532/567 [00:17<00:01, 30.18it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  94%|█████████▍| 533/567 [00:17<00:01, 30.19it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  94%|█████████▍| 534/567 [00:17<00:01, 30.19it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  94%|█████████▍| 535/567 [00:17<00:01, 30.21it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  95%|█████████▍| 536/567 [00:17<00:01, 30.23it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  95%|█████████▍| 537/567 [00:17<00:00, 30.26it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  95%|█████████▍| 538/567 [00:17<00:00, 30.28it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  95%|█████████▌| 539/567 [00:17<00:00, 30.29it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  95%|█████████▌| 540/567 [00:17<00:00, 30.31it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  95%|█████████▌| 541/567 [00:17<00:00, 30.34it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  96%|█████████▌| 542/567 [00:17<00:00, 30.36it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  96%|█████████▌| 543/567 [00:17<00:00, 30.39it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  96%|█████████▌| 544/567 [00:17<00:00, 30.42it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  96%|█████████▌| 545/567 [00:17<00:00, 30.44it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  96%|█████████▋| 546/567 [00:17<00:00, 30.46it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  96%|█████████▋| 547/567 [00:17<00:00, 30.49it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  97%|█████████▋| 548/567 [00:17<00:00, 30.52it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  97%|█████████▋| 549/567 [00:17<00:00, 30.54it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  97%|█████████▋| 550/567 [00:17<00:00, 30.56it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  97%|█████████▋| 551/567 [00:18<00:00, 30.57it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  97%|█████████▋| 552/567 [00:18<00:00, 30.58it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  98%|█████████▊| 553/567 [00:18<00:00, 30.61it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  98%|█████████▊| 554/567 [00:18<00:00, 30.63it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  98%|█████████▊| 555/567 [00:18<00:00, 30.66it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  98%|█████████▊| 556/567 [00:18<00:00, 30.68it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  98%|█████████▊| 557/567 [00:18<00:00, 30.70it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  98%|█████████▊| 558/567 [00:18<00:00, 30.73it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  99%|█████████▊| 559/567 [00:18<00:00, 30.74it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  99%|█████████▉| 560/567 [00:18<00:00, 30.77it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  99%|█████████▉| 561/567 [00:18<00:00, 30.80it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  99%|█████████▉| 562/567 [00:18<00:00, 30.83it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  99%|█████████▉| 563/567 [00:18<00:00, 30.86it/s, loss=0.129, v_num=4]\n",
      "Epoch 0:  99%|█████████▉| 564/567 [00:18<00:00, 30.90it/s, loss=0.129, v_num=4]\n",
      "Epoch 0: 100%|█████████▉| 565/567 [00:18<00:00, 30.94it/s, loss=0.129, v_num=4]\n",
      "Epoch 0: 100%|█████████▉| 566/567 [00:18<00:00, 30.97it/s, loss=0.129, v_num=4]\n",
      "Epoch 0: 100%|██████████| 567/567 [00:18<00:00, 30.99it/s, loss=0.129, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  80%|███████▉  | 453/567 [00:15<00:03, 29.75it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  80%|████████  | 454/567 [00:15<00:03, 29.05it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  80%|████████  | 455/567 [00:15<00:03, 29.08it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  80%|████████  | 456/567 [00:15<00:03, 29.08it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  81%|████████  | 457/567 [00:15<00:03, 29.08it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  81%|████████  | 458/567 [00:15<00:03, 29.10it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  81%|████████  | 459/567 [00:15<00:03, 29.12it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  81%|████████  | 460/567 [00:15<00:03, 29.14it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  81%|████████▏ | 461/567 [00:15<00:03, 29.16it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  81%|████████▏ | 462/567 [00:15<00:03, 29.18it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  82%|████████▏ | 463/567 [00:15<00:03, 29.20it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  82%|████████▏ | 464/567 [00:15<00:03, 29.23it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  82%|████████▏ | 465/567 [00:15<00:03, 29.25it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  82%|████████▏ | 466/567 [00:15<00:03, 29.28it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  82%|████████▏ | 467/567 [00:15<00:03, 29.32it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  83%|████████▎ | 468/567 [00:15<00:03, 29.35it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  83%|████████▎ | 469/567 [00:15<00:03, 29.37it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  83%|████████▎ | 470/567 [00:15<00:03, 29.39it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  83%|████████▎ | 471/567 [00:16<00:03, 29.42it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  83%|████████▎ | 472/567 [00:16<00:03, 29.45it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  83%|████████▎ | 473/567 [00:16<00:03, 29.46it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  84%|████████▎ | 474/567 [00:16<00:03, 29.50it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  84%|████████▍ | 475/567 [00:16<00:03, 29.53it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  84%|████████▍ | 476/567 [00:16<00:03, 29.55it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  84%|████████▍ | 477/567 [00:16<00:03, 29.57it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  84%|████████▍ | 478/567 [00:16<00:03, 29.58it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  84%|████████▍ | 479/567 [00:16<00:02, 29.60it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  85%|████████▍ | 480/567 [00:16<00:02, 29.62it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  85%|████████▍ | 481/567 [00:16<00:02, 29.64it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  85%|████████▌ | 482/567 [00:16<00:02, 29.67it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  85%|████████▌ | 483/567 [00:16<00:02, 29.70it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  85%|████████▌ | 484/567 [00:16<00:02, 29.73it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  86%|████████▌ | 485/567 [00:16<00:02, 29.75it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  86%|████████▌ | 486/567 [00:16<00:02, 29.77it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  86%|████████▌ | 487/567 [00:16<00:02, 29.80it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  86%|████████▌ | 488/567 [00:16<00:02, 29.81it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  86%|████████▌ | 489/567 [00:16<00:02, 29.83it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  86%|████████▋ | 490/567 [00:16<00:02, 29.85it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  87%|████████▋ | 491/567 [00:16<00:02, 29.86it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  87%|████████▋ | 492/567 [00:16<00:02, 29.89it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  87%|████████▋ | 493/567 [00:16<00:02, 29.91it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  87%|████████▋ | 494/567 [00:16<00:02, 29.93it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  87%|████████▋ | 495/567 [00:16<00:02, 29.96it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  87%|████████▋ | 496/567 [00:16<00:02, 29.98it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  88%|████████▊ | 497/567 [00:16<00:02, 29.99it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  88%|████████▊ | 498/567 [00:16<00:02, 30.02it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  88%|████████▊ | 499/567 [00:16<00:02, 30.05it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  88%|████████▊ | 500/567 [00:16<00:02, 30.07it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  88%|████████▊ | 501/567 [00:16<00:02, 30.08it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  89%|████████▊ | 502/567 [00:16<00:02, 30.12it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  89%|████████▊ | 503/567 [00:16<00:02, 30.15it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  89%|████████▉ | 504/567 [00:16<00:02, 30.17it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  89%|████████▉ | 505/567 [00:16<00:02, 30.20it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  89%|████████▉ | 506/567 [00:16<00:02, 30.21it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  89%|████████▉ | 507/567 [00:16<00:01, 30.23it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  90%|████████▉ | 508/567 [00:16<00:01, 30.25it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  90%|████████▉ | 509/567 [00:16<00:01, 30.26it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  90%|████████▉ | 510/567 [00:16<00:01, 30.29it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  90%|█████████ | 511/567 [00:16<00:01, 30.30it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  90%|█████████ | 512/567 [00:16<00:01, 30.33it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  90%|█████████ | 513/567 [00:16<00:01, 30.36it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  91%|█████████ | 514/567 [00:16<00:01, 30.37it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  91%|█████████ | 515/567 [00:16<00:01, 30.39it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  91%|█████████ | 516/567 [00:16<00:01, 30.42it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  91%|█████████ | 517/567 [00:16<00:01, 30.45it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  91%|█████████▏| 518/567 [00:17<00:01, 30.47it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  92%|█████████▏| 519/567 [00:17<00:01, 30.50it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  92%|█████████▏| 520/567 [00:17<00:01, 30.52it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  92%|█████████▏| 521/567 [00:17<00:01, 30.55it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  92%|█████████▏| 522/567 [00:17<00:01, 30.56it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  92%|█████████▏| 523/567 [00:17<00:01, 30.59it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  92%|█████████▏| 524/567 [00:17<00:01, 30.61it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  93%|█████████▎| 525/567 [00:17<00:01, 30.64it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  93%|█████████▎| 526/567 [00:17<00:01, 30.67it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  93%|█████████▎| 527/567 [00:17<00:01, 30.69it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  93%|█████████▎| 528/567 [00:17<00:01, 30.71it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  93%|█████████▎| 529/567 [00:17<00:01, 30.72it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  93%|█████████▎| 530/567 [00:17<00:01, 30.74it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  94%|█████████▎| 531/567 [00:17<00:01, 30.76it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  94%|█████████▍| 532/567 [00:17<00:01, 30.78it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  94%|█████████▍| 533/567 [00:17<00:01, 30.78it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  94%|█████████▍| 534/567 [00:17<00:01, 30.81it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  94%|█████████▍| 535/567 [00:17<00:01, 30.83it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  95%|█████████▍| 536/567 [00:17<00:01, 30.84it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  95%|█████████▍| 537/567 [00:17<00:00, 30.86it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  95%|█████████▍| 538/567 [00:17<00:00, 30.88it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  95%|█████████▌| 539/567 [00:17<00:00, 30.90it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  95%|█████████▌| 540/567 [00:17<00:00, 30.93it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  95%|█████████▌| 541/567 [00:17<00:00, 30.96it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  96%|█████████▌| 542/567 [00:17<00:00, 30.98it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  96%|█████████▌| 543/567 [00:17<00:00, 31.01it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  96%|█████████▌| 544/567 [00:17<00:00, 31.03it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  96%|█████████▌| 545/567 [00:17<00:00, 31.03it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  96%|█████████▋| 546/567 [00:17<00:00, 31.05it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  96%|█████████▋| 547/567 [00:17<00:00, 31.08it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  97%|█████████▋| 548/567 [00:17<00:00, 31.10it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  97%|█████████▋| 549/567 [00:17<00:00, 31.13it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  97%|█████████▋| 550/567 [00:17<00:00, 31.15it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  97%|█████████▋| 551/567 [00:17<00:00, 31.18it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  97%|█████████▋| 552/567 [00:17<00:00, 31.22it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  98%|█████████▊| 553/567 [00:17<00:00, 31.24it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  98%|█████████▊| 554/567 [00:17<00:00, 31.25it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  98%|█████████▊| 555/567 [00:17<00:00, 31.28it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  98%|█████████▊| 556/567 [00:17<00:00, 31.31it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  98%|█████████▊| 557/567 [00:17<00:00, 31.32it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  98%|█████████▊| 558/567 [00:17<00:00, 31.34it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  99%|█████████▊| 559/567 [00:17<00:00, 31.36it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  99%|█████████▉| 560/567 [00:17<00:00, 31.40it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  99%|█████████▉| 561/567 [00:17<00:00, 31.43it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  99%|█████████▉| 562/567 [00:17<00:00, 31.47it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  99%|█████████▉| 563/567 [00:17<00:00, 31.51it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1:  99%|█████████▉| 564/567 [00:17<00:00, 31.55it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1: 100%|█████████▉| 565/567 [00:17<00:00, 31.59it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1: 100%|█████████▉| 566/567 [00:17<00:00, 31.62it/s, loss=0.117, v_num=4, val/rmse=0.365, val/rmse_best=0.365]\n",
      "Epoch 1: 100%|██████████| 567/567 [00:17<00:00, 31.65it/s, loss=0.117, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  80%|███████▉  | 453/567 [00:15<00:03, 30.18it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  80%|████████  | 454/567 [00:15<00:03, 29.41it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  80%|████████  | 455/567 [00:15<00:03, 29.43it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  80%|████████  | 456/567 [00:15<00:03, 29.46it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  81%|████████  | 457/567 [00:15<00:03, 29.46it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  81%|████████  | 458/567 [00:15<00:03, 29.49it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  81%|████████  | 459/567 [00:15<00:03, 29.51it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  81%|████████  | 460/567 [00:15<00:03, 29.54it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  81%|████████▏ | 461/567 [00:15<00:03, 29.54it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  81%|████████▏ | 462/567 [00:15<00:03, 29.58it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  82%|████████▏ | 463/567 [00:15<00:03, 29.60it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  82%|████████▏ | 464/567 [00:15<00:03, 29.63it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  82%|████████▏ | 465/567 [00:15<00:03, 29.66it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  82%|████████▏ | 466/567 [00:15<00:03, 29.68it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  82%|████████▏ | 467/567 [00:15<00:03, 29.71it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  83%|████████▎ | 468/567 [00:15<00:03, 29.73it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  83%|████████▎ | 469/567 [00:15<00:03, 29.76it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  83%|████████▎ | 470/567 [00:15<00:03, 29.78it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  83%|████████▎ | 471/567 [00:15<00:03, 29.81it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  83%|████████▎ | 472/567 [00:15<00:03, 29.84it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  83%|████████▎ | 473/567 [00:15<00:03, 29.86it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  84%|████████▎ | 474/567 [00:15<00:03, 29.88it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  84%|████████▍ | 475/567 [00:15<00:03, 29.90it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  84%|████████▍ | 476/567 [00:15<00:03, 29.93it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  84%|████████▍ | 477/567 [00:15<00:03, 29.95it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  84%|████████▍ | 478/567 [00:15<00:02, 29.97it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  84%|████████▍ | 479/567 [00:15<00:02, 29.99it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  85%|████████▍ | 480/567 [00:16<00:02, 30.00it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  85%|████████▍ | 481/567 [00:16<00:02, 30.02it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  85%|████████▌ | 482/567 [00:16<00:02, 30.04it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  85%|████████▌ | 483/567 [00:16<00:02, 30.06it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  85%|████████▌ | 484/567 [00:16<00:02, 30.08it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  86%|████████▌ | 485/567 [00:16<00:02, 30.10it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  86%|████████▌ | 486/567 [00:16<00:02, 30.12it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  86%|████████▌ | 487/567 [00:16<00:02, 30.14it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  86%|████████▌ | 488/567 [00:16<00:02, 30.16it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  86%|████████▌ | 489/567 [00:16<00:02, 30.18it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  86%|████████▋ | 490/567 [00:16<00:02, 30.21it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  87%|████████▋ | 491/567 [00:16<00:02, 30.23it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  87%|████████▋ | 492/567 [00:16<00:02, 30.26it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  87%|████████▋ | 493/567 [00:16<00:02, 30.28it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  87%|████████▋ | 494/567 [00:16<00:02, 30.30it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  87%|████████▋ | 495/567 [00:16<00:02, 30.32it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  87%|████████▋ | 496/567 [00:16<00:02, 30.34it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  88%|████████▊ | 497/567 [00:16<00:02, 30.37it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  88%|████████▊ | 498/567 [00:16<00:02, 30.39it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  88%|████████▊ | 499/567 [00:16<00:02, 30.43it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  88%|████████▊ | 500/567 [00:16<00:02, 30.45it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  88%|████████▊ | 501/567 [00:16<00:02, 30.48it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  89%|████████▊ | 502/567 [00:16<00:02, 30.50it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  89%|████████▊ | 503/567 [00:16<00:02, 30.53it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  89%|████████▉ | 504/567 [00:16<00:02, 30.55it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  89%|████████▉ | 505/567 [00:16<00:02, 30.58it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  89%|████████▉ | 506/567 [00:16<00:01, 30.60it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  89%|████████▉ | 507/567 [00:16<00:01, 30.64it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  90%|████████▉ | 508/567 [00:16<00:01, 30.67it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  90%|████████▉ | 509/567 [00:16<00:01, 30.70it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  90%|████████▉ | 510/567 [00:16<00:01, 30.72it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  90%|█████████ | 511/567 [00:16<00:01, 30.73it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  90%|█████████ | 512/567 [00:16<00:01, 30.74it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  90%|█████████ | 513/567 [00:16<00:01, 30.75it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  91%|█████████ | 514/567 [00:16<00:01, 30.78it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  91%|█████████ | 515/567 [00:16<00:01, 30.80it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  91%|█████████ | 516/567 [00:16<00:01, 30.82it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  91%|█████████ | 517/567 [00:16<00:01, 30.85it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  91%|█████████▏| 518/567 [00:16<00:01, 30.86it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  92%|█████████▏| 519/567 [00:16<00:01, 30.88it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  92%|█████████▏| 520/567 [00:16<00:01, 30.90it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  92%|█████████▏| 521/567 [00:16<00:01, 30.92it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  92%|█████████▏| 522/567 [00:16<00:01, 30.93it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  92%|█████████▏| 523/567 [00:16<00:01, 30.96it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  92%|█████████▏| 524/567 [00:16<00:01, 30.96it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  93%|█████████▎| 525/567 [00:16<00:01, 30.99it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  93%|█████████▎| 526/567 [00:16<00:01, 31.01it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  93%|█████████▎| 527/567 [00:16<00:01, 31.03it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  93%|█████████▎| 528/567 [00:17<00:01, 31.05it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  93%|█████████▎| 529/567 [00:17<00:01, 31.08it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  93%|█████████▎| 530/567 [00:17<00:01, 31.12it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  94%|█████████▎| 531/567 [00:17<00:01, 31.13it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  94%|█████████▍| 532/567 [00:17<00:01, 31.16it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  94%|█████████▍| 533/567 [00:17<00:01, 31.19it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  94%|█████████▍| 534/567 [00:17<00:01, 31.21it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  94%|█████████▍| 535/567 [00:17<00:01, 31.23it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  95%|█████████▍| 536/567 [00:17<00:00, 31.25it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  95%|█████████▍| 537/567 [00:17<00:00, 31.28it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  95%|█████████▍| 538/567 [00:17<00:00, 31.31it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  95%|█████████▌| 539/567 [00:17<00:00, 31.33it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  95%|█████████▌| 540/567 [00:17<00:00, 31.35it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  95%|█████████▌| 541/567 [00:17<00:00, 31.38it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  96%|█████████▌| 542/567 [00:17<00:00, 31.41it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  96%|█████████▌| 543/567 [00:17<00:00, 31.44it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  96%|█████████▌| 544/567 [00:17<00:00, 31.46it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  96%|█████████▌| 545/567 [00:17<00:00, 31.49it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  96%|█████████▋| 546/567 [00:17<00:00, 31.53it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  96%|█████████▋| 547/567 [00:17<00:00, 31.56it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  97%|█████████▋| 548/567 [00:17<00:00, 31.58it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  97%|█████████▋| 549/567 [00:17<00:00, 31.61it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  97%|█████████▋| 550/567 [00:17<00:00, 31.62it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  97%|█████████▋| 551/567 [00:17<00:00, 31.64it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  97%|█████████▋| 552/567 [00:17<00:00, 31.65it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  98%|█████████▊| 553/567 [00:17<00:00, 31.67it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  98%|█████████▊| 554/567 [00:17<00:00, 31.70it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  98%|█████████▊| 555/567 [00:17<00:00, 31.72it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  98%|█████████▊| 556/567 [00:17<00:00, 31.74it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  98%|█████████▊| 557/567 [00:17<00:00, 31.76it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  98%|█████████▊| 558/567 [00:17<00:00, 31.79it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  99%|█████████▊| 559/567 [00:17<00:00, 31.82it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  99%|█████████▉| 560/567 [00:17<00:00, 31.84it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  99%|█████████▉| 561/567 [00:17<00:00, 31.87it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  99%|█████████▉| 562/567 [00:17<00:00, 31.91it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  99%|█████████▉| 563/567 [00:17<00:00, 31.93it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2:  99%|█████████▉| 564/567 [00:17<00:00, 31.97it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2: 100%|█████████▉| 565/567 [00:17<00:00, 32.00it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2: 100%|█████████▉| 566/567 [00:17<00:00, 32.04it/s, loss=0.089, v_num=4, val/rmse=0.354, val/rmse_best=0.354]\n",
      "Epoch 2: 100%|██████████| 567/567 [00:17<00:00, 32.06it/s, loss=0.089, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  80%|███████▉  | 453/567 [00:14<00:03, 31.43it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/114 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  80%|████████  | 454/567 [00:14<00:03, 30.64it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  80%|████████  | 455/567 [00:14<00:03, 30.66it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  80%|████████  | 456/567 [00:14<00:03, 30.67it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  81%|████████  | 457/567 [00:14<00:03, 30.62it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  81%|████████  | 458/567 [00:14<00:03, 30.64it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  81%|████████  | 459/567 [00:14<00:03, 30.67it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  81%|████████  | 460/567 [00:14<00:03, 30.70it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  81%|████████▏ | 461/567 [00:14<00:03, 30.74it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  81%|████████▏ | 462/567 [00:15<00:03, 30.76it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  82%|████████▏ | 463/567 [00:15<00:03, 30.79it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  82%|████████▏ | 464/567 [00:15<00:03, 30.82it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  82%|████████▏ | 465/567 [00:15<00:03, 30.84it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  82%|████████▏ | 466/567 [00:15<00:03, 30.87it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  82%|████████▏ | 467/567 [00:15<00:03, 30.89it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  83%|████████▎ | 468/567 [00:15<00:03, 30.91it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  83%|████████▎ | 469/567 [00:15<00:03, 30.93it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  83%|████████▎ | 470/567 [00:15<00:03, 30.95it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  83%|████████▎ | 471/567 [00:15<00:03, 30.95it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  83%|████████▎ | 472/567 [00:15<00:03, 30.97it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  83%|████████▎ | 473/567 [00:15<00:03, 31.00it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  84%|████████▎ | 474/567 [00:15<00:02, 31.02it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  84%|████████▍ | 475/567 [00:15<00:02, 31.04it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  84%|████████▍ | 476/567 [00:15<00:02, 31.07it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  84%|████████▍ | 477/567 [00:15<00:02, 31.09it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  84%|████████▍ | 478/567 [00:15<00:02, 31.12it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  84%|████████▍ | 479/567 [00:15<00:02, 31.14it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  85%|████████▍ | 480/567 [00:15<00:02, 31.17it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  85%|████████▍ | 481/567 [00:15<00:02, 31.18it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  85%|████████▌ | 482/567 [00:15<00:02, 31.21it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  85%|████████▌ | 483/567 [00:15<00:02, 31.24it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  85%|████████▌ | 484/567 [00:15<00:02, 31.26it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  86%|████████▌ | 485/567 [00:15<00:02, 31.29it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  86%|████████▌ | 486/567 [00:15<00:02, 31.31it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  86%|████████▌ | 487/567 [00:15<00:02, 31.32it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  86%|████████▌ | 488/567 [00:15<00:02, 31.33it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  86%|████████▌ | 489/567 [00:15<00:02, 31.36it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  86%|████████▋ | 490/567 [00:15<00:02, 31.38it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  87%|████████▋ | 491/567 [00:15<00:02, 31.41it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  87%|████████▋ | 492/567 [00:15<00:02, 31.43it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  87%|████████▋ | 493/567 [00:15<00:02, 31.46it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  87%|████████▋ | 494/567 [00:15<00:02, 31.48it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  87%|████████▋ | 495/567 [00:15<00:02, 31.50it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  87%|████████▋ | 496/567 [00:15<00:02, 31.53it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  88%|████████▊ | 497/567 [00:15<00:02, 31.55it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  88%|████████▊ | 498/567 [00:15<00:02, 31.58it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  88%|████████▊ | 499/567 [00:15<00:02, 31.61it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  88%|████████▊ | 500/567 [00:15<00:02, 31.61it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  88%|████████▊ | 501/567 [00:15<00:02, 31.64it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  89%|████████▊ | 502/567 [00:15<00:02, 31.66it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  89%|████████▊ | 503/567 [00:15<00:02, 31.68it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  89%|████████▉ | 504/567 [00:15<00:01, 31.71it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  89%|████████▉ | 505/567 [00:15<00:01, 31.74it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  89%|████████▉ | 506/567 [00:15<00:01, 31.76it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  89%|████████▉ | 507/567 [00:15<00:01, 31.79it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  90%|████████▉ | 508/567 [00:15<00:01, 31.82it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  90%|████████▉ | 509/567 [00:15<00:01, 31.85it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  90%|████████▉ | 510/567 [00:15<00:01, 31.88it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  90%|█████████ | 511/567 [00:16<00:01, 31.90it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  90%|█████████ | 512/567 [00:16<00:01, 31.93it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  90%|█████████ | 513/567 [00:16<00:01, 31.96it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  91%|█████████ | 514/567 [00:16<00:01, 31.98it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  91%|█████████ | 515/567 [00:16<00:01, 32.00it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  91%|█████████ | 516/567 [00:16<00:01, 32.03it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  91%|█████████ | 517/567 [00:16<00:01, 32.04it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  91%|█████████▏| 518/567 [00:16<00:01, 32.08it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  92%|█████████▏| 519/567 [00:16<00:01, 32.11it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  92%|█████████▏| 520/567 [00:16<00:01, 32.13it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  92%|█████████▏| 521/567 [00:16<00:01, 32.15it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  92%|█████████▏| 522/567 [00:16<00:01, 32.17it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  92%|█████████▏| 523/567 [00:16<00:01, 32.19it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  92%|█████████▏| 524/567 [00:16<00:01, 32.22it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  93%|█████████▎| 525/567 [00:16<00:01, 32.25it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  93%|█████████▎| 526/567 [00:16<00:01, 32.28it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  93%|█████████▎| 527/567 [00:16<00:01, 32.31it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  93%|█████████▎| 528/567 [00:16<00:01, 32.34it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  93%|█████████▎| 529/567 [00:16<00:01, 32.36it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  93%|█████████▎| 530/567 [00:16<00:01, 32.39it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  94%|█████████▎| 531/567 [00:16<00:01, 32.41it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  94%|█████████▍| 532/567 [00:16<00:01, 32.44it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  94%|█████████▍| 533/567 [00:16<00:01, 32.46it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  94%|█████████▍| 534/567 [00:16<00:01, 32.49it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  94%|█████████▍| 535/567 [00:16<00:00, 32.51it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  95%|█████████▍| 536/567 [00:16<00:00, 32.54it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  95%|█████████▍| 537/567 [00:16<00:00, 32.54it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  95%|█████████▍| 538/567 [00:16<00:00, 32.55it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  95%|█████████▌| 539/567 [00:16<00:00, 32.58it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  95%|█████████▌| 540/567 [00:16<00:00, 32.60it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  95%|█████████▌| 541/567 [00:16<00:00, 32.63it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  96%|█████████▌| 542/567 [00:16<00:00, 32.64it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  96%|█████████▌| 543/567 [00:16<00:00, 32.65it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  96%|█████████▌| 544/567 [00:16<00:00, 32.68it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  96%|█████████▌| 545/567 [00:16<00:00, 32.69it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  96%|█████████▋| 546/567 [00:16<00:00, 32.71it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  96%|█████████▋| 547/567 [00:16<00:00, 32.72it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  97%|█████████▋| 548/567 [00:16<00:00, 32.74it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  97%|█████████▋| 549/567 [00:16<00:00, 32.75it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  97%|█████████▋| 550/567 [00:16<00:00, 32.77it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  97%|█████████▋| 551/567 [00:16<00:00, 32.78it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  97%|█████████▋| 552/567 [00:16<00:00, 32.79it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  98%|█████████▊| 553/567 [00:16<00:00, 32.82it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  98%|█████████▊| 554/567 [00:16<00:00, 32.84it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  98%|█████████▊| 555/567 [00:16<00:00, 32.85it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  98%|█████████▊| 556/567 [00:16<00:00, 32.86it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  98%|█████████▊| 557/567 [00:16<00:00, 32.89it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  98%|█████████▊| 558/567 [00:16<00:00, 32.89it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  99%|█████████▊| 559/567 [00:16<00:00, 32.90it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  99%|█████████▉| 560/567 [00:17<00:00, 32.93it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  99%|█████████▉| 561/567 [00:17<00:00, 32.97it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  99%|█████████▉| 562/567 [00:17<00:00, 33.01it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  99%|█████████▉| 563/567 [00:17<00:00, 33.05it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3:  99%|█████████▉| 564/567 [00:17<00:00, 33.09it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3: 100%|█████████▉| 565/567 [00:17<00:00, 33.13it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3: 100%|█████████▉| 566/567 [00:17<00:00, 33.16it/s, loss=0.0826, v_num=4, val/rmse=0.385, val/rmse_best=0.354]\n",
      "Epoch 3: 100%|██████████| 567/567 [00:17<00:00, 33.19it/s, loss=0.0826, v_num=4, val/rmse=0.292, val/rmse_best=0.292]\n",
      "Epoch 4:  49%|████▉     | 278/567 [00:10<00:10, 27.58it/s, loss=0.0879, v_num=4, val/rmse=0.292, val/rmse_best=0.292]"
     ]
    }
   ],
   "source": [
    "train_data = TrainDataset()\n",
    "test_data = TestDataset()\n",
    "\n",
    "dm = BaseDataModule(train_data, test_data, batch_size=32, fold=0)\n",
    "\n",
    "net = GATNet()\n",
    "model = BaseNet(net, lr=1e-3, weight_decay=1e-5, max_epochs=30)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=30, gpus=[1])\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:11<00:00, 38.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.12264837247317463\n",
      "Val Loss: 0.11823252588510513\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 37.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1135789750690755\n",
      "Val Loss: 0.11279676854610443\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 37.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.09695399670181158\n",
      "Val Loss: 0.08205020427703857\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 36.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0808124654652925\n",
      "Val Loss: 0.09489650279283524\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 35.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.07588006033835558\n",
      "Val Loss: 0.08151715993881226\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 35.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.07270524785679171\n",
      "Val Loss: 0.07278463989496231\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:13<00:00, 34.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0707416209098256\n",
      "Val Loss: 0.07197262346744537\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:13<00:00, 33.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06920491806570662\n",
      "Val Loss: 0.07027502357959747\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:13<00:00, 33.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06744053234525074\n",
      "Val Loss: 0.07098870724439621\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:13<00:00, 33.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06588058465594224\n",
      "Val Loss: 0.06632950156927109\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 35.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06486200984921019\n",
      "Val Loss: 0.0678791031241417\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 34.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06371644361327027\n",
      "Val Loss: 0.06562931090593338\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:13<00:00, 34.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06215264864068552\n",
      "Val Loss: 0.06718893349170685\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:13<00:00, 34.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06086020463580064\n",
      "Val Loss: 0.061521340161561966\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:11<00:00, 38.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.059979709070874895\n",
      "Val Loss: 0.06226282939314842\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:11<00:00, 38.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.059344057733846815\n",
      "Val Loss: 0.0634029284119606\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 34.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05834461612499445\n",
      "Val Loss: 0.060086771845817566\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 36.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05679242212169086\n",
      "Val Loss: 0.05918416753411293\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 37.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05571853669635771\n",
      "Val Loss: 0.058793921023607254\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:11<00:00, 37.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.054527673138424784\n",
      "Val Loss: 0.05806384235620499\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:11<00:00, 37.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05377750939979459\n",
      "Val Loss: 0.058353133499622345\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 36.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05272164444539852\n",
      "Val Loss: 0.05835919827222824\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 35.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05194892519242058\n",
      "Val Loss: 0.057384613901376724\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 35.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05119385129091624\n",
      "Val Loss: 0.05701125040650368\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05046723569202634\n",
      "Val Loss: 0.05732369422912598\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 36.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0499665065777486\n",
      "Val Loss: 0.05713200941681862\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 35.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.04949362321487457\n",
      "Val Loss: 0.0570257306098938\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 36.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.04914069553183404\n",
      "Val Loss: 0.056887321174144745\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.04893808028605205\n",
      "Val Loss: 0.05693376436829567\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:12<00:00, 36.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.04880983686736614\n",
      "Val Loss: 0.056932494044303894\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "model = GATNet(15, 2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_dataloader))\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss, val_loss = 0., 0.\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        loss = criterion(pred, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        loss = criterion(pred, batch.y)\n",
    "        val_loss += loss * len(batch.y)\n",
    "    \n",
    "    val_loss /= len(val_data)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss}\")\n",
    "    print(f\"Val Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch = batch.to(device)\n",
    "    pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "    preds.append(pred)\n",
    "\n",
    "preds = torch.cat(preds).detach().cpu().numpy()\n",
    "\n",
    "sub_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "sub_df[\"Reorg_g\"] = preds[:, 0]\n",
    "sub_df[\"Reorg_ex\"] = preds[:, 1]\n",
    "sub_df.to_csv(\"submission.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mol",
   "language": "python",
   "name": "mol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
